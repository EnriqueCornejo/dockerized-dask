{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.dot import dot_graph\n",
    "import itertools\n",
    "import logging\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "from urllib import request\n",
    "from multiprocessing import Pool\n",
    "\n",
    "client = Client('scheduler:8786')\n",
    "\n",
    "download_location = '/temp'\n",
    "data_url = 'http://nasanex.s3.amazonaws.com'\n",
    "max_download_attempts = 5\n",
    "\n",
    "all_models = ['ACCESS1-0',  'BNU-ESM', 'CCSM4', 'CESM1-BGC', 'CNRM-CM5', 'CSIRO-Mk3-6-0', 'CanESM2', 'GFDL-CM3', 'GFDL-ESM2G', 'GFDL-ESM2M', 'IPSL-CM5A-LR', 'IPSL-CM5A-MR', 'MIROC-ESM-CHEM', 'MIROC-ESM', 'MIROC5', 'MPI-ESM-LR', 'MPI-ESM-MR', 'MRI-CGCM3', 'NorESM1-M', 'bcc-csm1-1', 'inmcm4']\n",
    "all_models = ['ACCESS1-0', 'BNU-ESM'] \n",
    "all_vars = ['tasmax', 'pr']\n",
    "all_years = {\n",
    "     # 'historical': list(range(1971, 1976))\n",
    "    'historical': list(range(1971, 2001))\n",
    "}\n",
    "\n",
    "def get_dataset_url(variable, scenario, model, year, prefix = data_url):\n",
    "    prefix_filename = '/'.join([prefix, 'NEX-GDDP', 'BCSD', scenario, 'day', 'atmos', variable, 'r1i1p1', 'v1.0'])\n",
    "    filename = '_'.join([variable, 'day', 'BCSD', scenario, 'r1i1p1', model, str(year) + '.nc'])\n",
    "    url = '/'.join([prefix_filename, filename])\n",
    "    return url\n",
    "\n",
    "def get_context(year, **kwargs):\n",
    "    variables = [kwargs.get('variable')] if kwargs.get('variable') else all_vars\n",
    "    scenarios = ['historical']\n",
    "    models = [kwargs.get('model')] if kwargs.get('model') else all_models\n",
    "    outlist = []\n",
    "    combinations = list(itertools.product(variables, scenarios, models))\n",
    "    result = list(map(lambda comb: [ *comb, year ], combinations))\n",
    "    return result\n",
    "\n",
    "def get_year_ensemble(year, variable = 'tasmax'):\n",
    "    context = get_context(year, variable = variable)\n",
    "    datasets = list(map(lambda x: str(get_dataset_url(*x)), context))\n",
    "    return datasets\n",
    "\n",
    "def download_file(url):\n",
    "    print(\"url: \" + url)\n",
    "    attempts = 0\n",
    "    success = False\n",
    "    filename = \"\"\n",
    "    while attempts < max_download_attempts and not success:\n",
    "        time.sleep(2 ** attempts)\n",
    "        filename = '/'.join([download_location, str(url.split('/')[-1])])\n",
    "        print(\"Downloading file at \" + filename)\n",
    "        u = request.urlopen(url)\n",
    "        f = open(filename, 'wb')\n",
    "        f.write(u.read())\n",
    "        f.close()\n",
    "        success = True\n",
    "        break\n",
    "    return filename\n",
    "\n",
    "def download_file_list(url_list):\n",
    "    print(\"Starting download pool\")\n",
    "    pool = Pool()\n",
    "    res = pool.map(download_file, url_list)\n",
    "    print(\"Jobs sent\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"Downloads finished\")\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "def download_and_stack(year, variable):\n",
    "    dsets_urls = list(map(lambda x: get_year_ensemble(x, variable = variable), [year]))[0]\n",
    "    filenames = download_file_list(dsets_urls)\n",
    "    datasets = [ netCDF4.Dataset(filename) for filename in filenames ]\n",
    "    dask_arrays = []\n",
    "    for dset in datasets:\n",
    "        dask_arrays.append(da.from_array(dset[str(variable)], chunks= (366, 144, 144)))\n",
    "    try:\n",
    "        final_stack = da.stack(dask_arrays, axis = 0)\n",
    "    except:\n",
    "        return filenames, None, None\n",
    "    return filenames, datasets, final_stack\n",
    "\n",
    "def get_stacks_mod_avg(a, chunksize):\n",
    "    nmodels, time, lat, lon = a.shape\n",
    "    nstacks_lat = int(np.ceil(lat / chunksize))\n",
    "    nstacks_lon = int(np.ceil(lon / chunksize))\n",
    "    \n",
    "    stacks = []\n",
    "    \n",
    "    for i in range(nstacks_lat):\n",
    "        for j in range(nstacks_lon):\n",
    "            latmin, latmax = i * chunksize, (i+1) * chunksize\n",
    "            lonmin, lonmax = j * chunksize, (j+1) * chunksize\n",
    "            print(i, j, '~>', latmin, latmax, lonmin, lonmax)\n",
    "            stacked = a[:, :, latmin:latmax, lonmin:lonmax]\n",
    "            print(stacked)\n",
    "            stacks.append(stacked)\n",
    "    return stacks\n",
    "\n",
    "def copy_dataset(src, output_filename):\n",
    "    dst = netCDF4.Dataset('/home/jovyan/work/' + output_filename, \"w\")\n",
    "    # copy attributes\n",
    "    for name in src.ncattrs():\n",
    "        dst.setncattr(name, src.getncattr(name))\n",
    "    # copy dimensions\n",
    "    for name, dimension in src.dimensions.items():\n",
    "        dst.createDimension(\n",
    "            name, (len(dimension) if not dimension.isunlimited else None))\n",
    "    dst.close()\n",
    "    return output_filename\n",
    "\n",
    "def restack(chunk_list, chunksize):\n",
    "    shapes = list(map(np.shape, chunk_list))\n",
    "    ndays = shapes[0][0]\n",
    "    nlons = int(1440 / chunksize)\n",
    "    nlats = int(720 / chunksize)\n",
    "\n",
    "    out_array = np.empty((ndays, 720, 1440))\n",
    " \n",
    "    combs = list(itertools.product(\n",
    "        list(range(nlats)),\n",
    "        list(range(nlons))\n",
    "    ))\n",
    "    \n",
    "    res_list = zip(combs, chunk_list)\n",
    "    \n",
    "    for position, arr in res_list:\n",
    "        minlon, maxlon = position[0] * chunksize, position[0] * chunksize + chunksize\n",
    "        minlat, maxlat = position[1] * chunksize, position[1] * chunksize + chunksize\n",
    "        out_array[:, minlon:maxlon, minlat:maxlat] = arr\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://scheduler:8786\n",
       "  <li><b>Dashboard: </b><a href='http://scheduler:8787' target='_blank'>http://scheduler:8787</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>33.69 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://172.22.0.2:8786' processes=1 cores=8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download pool\n",
      "url: http://nasanex.s3.amazonaws.com/NEX-GDDP/BCSD/historical/day/atmos/pr/r1i1p1/v1.0/pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1971.nc\n",
      "url: http://nasanex.s3.amazonaws.com/NEX-GDDP/BCSD/historical/day/atmos/pr/r1i1p1/v1.0/pr_day_BCSD_historical_r1i1p1_BNU-ESM_1971.nc\n",
      "Downloading file at /temp/pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1971.nc\n",
      "Downloading file at /temp/pr_day_BCSD_historical_r1i1p1_BNU-ESM_1971.nc\n"
     ]
    }
   ],
   "source": [
    "# Processing pr years\n",
    "years = list(range(1971, 2001))\n",
    "var = 'pr'\n",
    "for i, year in enumerate(years):\n",
    "    filenames, datasets, current_year_stack = download_and_stack(year, variable=var)\n",
    "    if current_year_stack is not None:\n",
    "        chunked_stack = get_stacks_mod_avg(current_year_stack, 360)\n",
    "        mean = list(map(lambda x: delayed(np.mean)(x, axis=0).compute(), chunked_stack))\n",
    "        out_array = restack(mean, 360)\n",
    "        # np.save('/home/jovyan/work/' + str(year) + '_pr_avg.npy', out_array)\n",
    "        del chunked_stack\n",
    "        del mean\n",
    "        del out_array\n",
    "        del current_year_stack\n",
    "        #output_dataset_filename = copy_dataset(datasets[0], str(year) + '_ensemble_' + var + '.nc')\n",
    "        #output_dataset = netCDF4.Dataset(output_dataset_filename, 'w')\n",
    "        #print(output_dataset)\n",
    "        \n",
    "    # Finally\n",
    "    for filename in filenames:\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 68000896\r\n",
      "-rw-r--r-- 1 jovyan users         96 Nov 21 22:01 1971_ensemble_pr.nc\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 21 22:55 1971_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 21 23:32 1973_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 00:07 1974_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 00:43 1975_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 01:20 1977_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 01:55 1978_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 02:31 1979_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 03:08 1981_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 03:44 1982_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 04:21 1983_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 04:57 1985_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 05:32 1986_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 06:08 1987_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 06:45 1989_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 07:21 1990_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 07:57 1991_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 08:36 1993_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 09:12 1994_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 09:49 1995_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 10:25 1997_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 11:01 1998_pr_avg.npy\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 22 11:38 1999_pr_avg.npy\r\n",
      "-rw-rw-r-- 1 jovyan users     181924 Nov 21 19:05 dask.ipynb\r\n",
      "-rw-r--r-- 1 jovyan users          0 Nov 21 21:25 hello.txt\r\n",
      "-rw-rw-r-- 1 jovyan users     829271 Nov 21 10:37 mydask.png\r\n",
      "-rw-r--r-- 1 jovyan users 3027456096 Nov 21 18:52 pr_test.npy\r\n",
      "-rw-r--r-- 1 jovyan users     219636 Nov 22 11:41 Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
