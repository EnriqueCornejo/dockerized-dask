{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://scheduler:8786\n",
       "  <li><b>Dashboard: </b><a href='http://scheduler:8787' target='_blank'>http://scheduler:8787</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>33.69 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://172.22.0.2:8786' processes=0 cores=0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.dot import dot_graph\n",
    "import itertools\n",
    "import logging\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "from urllib import request\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "\n",
    "client = Client('scheduler:8786')\n",
    "\n",
    "download_location = '/temp'\n",
    "data_url = 'http://nasanex.s3.amazonaws.com'\n",
    "# data_url = 'http://172.21.0.1:8080'\n",
    "max_download_attempts = 5\n",
    "\n",
    "all_models = ['ACCESS1-0',  'BNU-ESM', 'CCSM4', 'CESM1-BGC', 'CNRM-CM5', 'CSIRO-Mk3-6-0', 'CanESM2', 'GFDL-CM3', 'GFDL-ESM2G', 'GFDL-ESM2M', 'IPSL-CM5A-LR', 'IPSL-CM5A-MR', 'MIROC-ESM-CHEM', 'MIROC-ESM', 'MIROC5', 'MPI-ESM-LR', 'MPI-ESM-MR', 'MRI-CGCM3', 'NorESM1-M', 'bcc-csm1-1', 'inmcm4']\n",
    "# all_models = ['ACCESS1-0', 'BNU-ESM'] \n",
    "all_vars = ['tasmax', 'pr']\n",
    "all_years = {\n",
    "     # 'historical': list(range(1971, 1976))\n",
    "    'historical': list(range(1971, 2001))\n",
    "}\n",
    "\n",
    "def get_dataset_url(variable, scenario, model, year, prefix = data_url):\n",
    "    prefix_filename = '/'.join([prefix, 'NEX-GDDP', 'BCSD', scenario, 'day', 'atmos', variable, 'r1i1p1', 'v1.0'])\n",
    "    filename = '_'.join([variable, 'day', 'BCSD', scenario, 'r1i1p1', model, str(year) + '.nc'])\n",
    "    url = '/'.join([prefix_filename, filename])\n",
    "    return url\n",
    "\n",
    "def get_context(year, **kwargs):\n",
    "    variables = [kwargs.get('variable')] if kwargs.get('variable') else all_vars\n",
    "    scenarios = ['historical']\n",
    "    models = [kwargs.get('model')] if kwargs.get('model') else all_models\n",
    "    outlist = []\n",
    "    combinations = list(itertools.product(variables, scenarios, models))\n",
    "    result = list(map(lambda comb: [ *comb, year ], combinations))\n",
    "    return result\n",
    "\n",
    "def get_year_ensemble(year, variable = 'tasmax'):\n",
    "    context = get_context(year, variable = variable)\n",
    "    datasets = list(map(lambda x: str(get_dataset_url(*x)), context))\n",
    "    return datasets\n",
    "\n",
    "def download_file(url):\n",
    "    print(\"url: \" + url)\n",
    "    attempts = 0\n",
    "    success = False\n",
    "    filename = \"\"\n",
    "    while attempts < max_download_attempts and not success:\n",
    "        time.sleep(2 ** attempts)\n",
    "        filename = '/'.join([download_location, str(url.split('/')[-1])])\n",
    "        print(\"Downloading file at \" + filename)\n",
    "        u = request.urlopen(url)\n",
    "        f = open(filename, 'wb')\n",
    "        f.write(u.read())\n",
    "        f.close()\n",
    "        success = True\n",
    "        break\n",
    "    return filename\n",
    "\n",
    "def download_file_list(url_list):\n",
    "    print(\"Starting download pool\")\n",
    "    pool = Pool()\n",
    "    res = pool.map(download_file, url_list)\n",
    "    print(\"Jobs sent\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"Downloads finished\")\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "def download_and_stack(year, variable):\n",
    "    dsets_urls = list(map(lambda x: get_year_ensemble(x, variable = variable), [year]))[0]\n",
    "    filenames = download_file_list(dsets_urls)\n",
    "    datasets = [ netCDF4.Dataset(filename) for filename in filenames ]\n",
    "    dask_arrays = []\n",
    "    for dset in datasets:\n",
    "        dask_arrays.append(da.from_array(dset[str(variable)], chunks= (366, 144, 144)))\n",
    "    try:\n",
    "        final_stack = da.stack(dask_arrays, axis = 0)\n",
    "    except:\n",
    "        return filenames, None, None\n",
    "    return filenames, datasets, final_stack\n",
    "\n",
    "def get_stacks_mod_avg(a, chunksize):\n",
    "    nmodels, time, lat, lon = a.shape\n",
    "    nstacks_lat = int(np.ceil(lat / chunksize))\n",
    "    nstacks_lon = int(np.ceil(lon / chunksize))\n",
    "    \n",
    "    stacks = []\n",
    "    \n",
    "    for i in range(nstacks_lat):\n",
    "        for j in range(nstacks_lon):\n",
    "            latmin, latmax = i * chunksize, (i+1) * chunksize\n",
    "            lonmin, lonmax = j * chunksize, (j+1) * chunksize\n",
    "            print(i, j, '~>', latmin, latmax, lonmin, lonmax)\n",
    "            stacked = a[:, :, latmin:latmax, lonmin:lonmax]\n",
    "            print(stacked)\n",
    "            stacks.append(stacked)\n",
    "    return stacks\n",
    "\n",
    "def copy_dataset(src, output_filename):\n",
    "    dst = netCDF4.Dataset('/home/jovyan/work/' + output_filename, \"w\")\n",
    "    # copy attributes\n",
    "    for name in src.ncattrs():\n",
    "        dst.setncattr(name, src.getncattr(name))\n",
    "    # copy dimensions\n",
    "    for name, dimension in src.dimensions.items():\n",
    "        dst.createDimension(\n",
    "            name, (dimension)\n",
    "        )\n",
    "    dst.close()\n",
    "    return output_filename\n",
    "\n",
    "def restack(chunk_list, chunksize):\n",
    "    shapes = list(map(np.shape, chunk_list))\n",
    "    ndays = shapes[0][0]\n",
    "    nlons = int(1440 / chunksize)\n",
    "    nlats = int(720 / chunksize)\n",
    "    out_array = np.empty((ndays, 720, 1440))\n",
    "    combs = list(itertools.product(\n",
    "        list(range(nlats)),\n",
    "        list(range(nlons))\n",
    "    ))\n",
    "    res_list = zip(combs, chunk_list)\n",
    "    for position, arr in res_list:\n",
    "        minlon, maxlon = position[0] * chunksize, position[0] * chunksize + chunksize\n",
    "        minlat, maxlat = position[1] * chunksize, position[1] * chunksize + chunksize\n",
    "        out_array[:, minlon:maxlon, minlat:maxlat] = arr\n",
    "    return out_array\n",
    "\n",
    "def stack_from_disk(year):\n",
    "    year_ensemble_tasmin = list(map(lambda x: x.split('/')[-1], get_year_ensemble(year, variable = 'tasmin')))\n",
    "    year_ensemble_tasmax = list(map(lambda x: x.split('/')[-1], get_year_ensemble(year, variable = 'tasmax')))\n",
    "    datasets_tasmin = [ netCDF4.Dataset('../temp/' + filename) for filename in year_ensemble_tasmin ]\n",
    "    datasets_tasmax = [ netCDF4.Dataset('../temp/' + filename) for filename in year_ensemble_tasmax ]\n",
    "    das_tasmin = list(map(lambda dset: da.from_array(dset['tasmin'], chunks = (365, 360, 360)), datasets_tasmin))\n",
    "    das_tasmax = list(map(lambda dset: da.from_array(dset['tasmin'], chunks = (365, 360, 360)), datasets_tasmin))\n",
    "    # da.stack(list(map(lambda dset: da.from_array(dset['tasmax'], chunks = (365, 144, 144)), datasets_tasmax)))\n",
    "    final_stack = da.stack([da.stack(tup) for tup in list(zip(das_tasmin, das_tasmax))])\n",
    "    return final_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stack_1971 = stack_from_disk(1971)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array<stack, shape=(21, 2, 365, 720, 1440), dtype=float32, chunksize=(1, 1, 365, 360, 360)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_1971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 31), (31, 59), (59, 90), (90, 120), (120, 151), (151, 181), (181, 212), (212, 243), (243, 273), (273, 304), (304, 334), (334, 365)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dask.array<getitem, shape=(21, 2, 31, 720, 1440), dtype=float32, chunksize=(1, 1, 31, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 28, 720, 1440), dtype=float32, chunksize=(1, 1, 28, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 31, 720, 1440), dtype=float32, chunksize=(1, 1, 31, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 30, 720, 1440), dtype=float32, chunksize=(1, 1, 30, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 31, 720, 1440), dtype=float32, chunksize=(1, 1, 31, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 30, 720, 1440), dtype=float32, chunksize=(1, 1, 30, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 31, 720, 1440), dtype=float32, chunksize=(1, 1, 31, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 31, 720, 1440), dtype=float32, chunksize=(1, 1, 31, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 30, 720, 1440), dtype=float32, chunksize=(1, 1, 30, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 31, 720, 1440), dtype=float32, chunksize=(1, 1, 31, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 30, 720, 1440), dtype=float32, chunksize=(1, 1, 30, 360, 360)>,\n",
       " dask.array<getitem, shape=(21, 2, 31, 720, 1440), dtype=float32, chunksize=(1, 1, 31, 360, 360)>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "year_leap_days = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "def days_to_ranges(years):\n",
    "    ranges = [year_days[:i +1] for i, n in enumerate(year_days)]\n",
    "    result = [(sum(element[:-1]), sum(element)) for element in ranges]\n",
    "    return result\n",
    "\n",
    "def stack_to_months(stack):\n",
    "    days = days_to_ranges(year_days) if stack.shape[2] == 365 else days_to_ranges(year_leap_days)\n",
    "    for period in days:\n",
    "        yield(stack[:, :, period[0]:period[1], :, :])\n",
    "\n",
    "list(stack_to_months(stack_1971))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 31), (31, 59), (59, 90), (90, 120), (120, 151), (151, 181), (181, 212), (212, 243), (243, 273), (273, 304), (304, 334), (334, 365)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1243387e+18,\n",
       " 1.1243386e+18,\n",
       " 1.1243387e+18,\n",
       " 1.1243386e+18,\n",
       " 1.1243387e+18,\n",
       " 1.1243386e+18,\n",
       " 1.1243387e+18,\n",
       " 1.1243387e+18,\n",
       " 1.1243386e+18,\n",
       " 1.1243387e+18,\n",
       " 1.1366843e+18,\n",
       " 1.1258818e+18]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_stacks = stack_to_months(stack_1971)\n",
    "results = [ np.mean(month).compute() for month in month_stacks ]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "process_temperatures.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
