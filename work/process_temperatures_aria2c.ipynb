{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.dot import dot_graph\n",
    "import itertools\n",
    "import logging\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "from urllib import request\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "import tempfile\n",
    "import subprocess\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We define some important data\n",
    "client = Client('scheduler:8786')\n",
    "download_location = '/temp'\n",
    "data_url = 'http://nasanex.s3.amazonaws.com'\n",
    "data_url = 'http://172.21.0.1:8080'\n",
    "max_download_attempts = 5\n",
    "all_models = ['ACCESS1-0',  'BNU-ESM', 'CCSM4', 'CESM1-BGC', 'CNRM-CM5', 'CSIRO-Mk3-6-0', 'CanESM2', 'GFDL-CM3', 'GFDL-ESM2G', 'GFDL-ESM2M', 'IPSL-CM5A-LR', 'IPSL-CM5A-MR', 'MIROC-ESM-CHEM', 'MIROC-ESM', 'MIROC5', 'MPI-ESM-LR', 'MPI-ESM-MR', 'MRI-CGCM3', 'NorESM1-M', 'bcc-csm1-1', 'inmcm4']\n",
    "# all_models = ['ACCESS1-0',  'BNU-ESM', 'bcc-csm1-1']\n",
    "all_vars = ['tasmax', 'pr', 'tasmin']\n",
    "all_years = {\n",
    "    'historical': list(range(1971, 2001))\n",
    "}\n",
    "\n",
    "year_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "year_leap_days = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# And some functions to deal with loading data into the cluster\n",
    "def get_dataset_url(variable, scenario, model, year, prefix = data_url):\n",
    "    prefix_filename = '/'.join([prefix, 'NEX-GDDP', 'BCSD', scenario, 'day', 'atmos', variable, 'r1i1p1', 'v1.0'])\n",
    "    filename = '_'.join([variable, 'day', 'BCSD', scenario, 'r1i1p1', model, str(year) + '.nc'])\n",
    "    url = '/'.join([prefix_filename, filename])\n",
    "    # url = '/'.join([prefix, filename])\n",
    "    return url\n",
    "\n",
    "def get_context(year, **kwargs):\n",
    "    variables = [kwargs.get('variable')] if kwargs.get('variable') else all_vars\n",
    "    scenarios = ['historical']\n",
    "    models = [kwargs.get('model')] if kwargs.get('model') else all_models\n",
    "    outlist = []\n",
    "    combinations = list(itertools.product(variables, scenarios, models))\n",
    "    result = list(map(lambda comb: [ *comb, year ], combinations))\n",
    "    return result\n",
    "\n",
    "def get_year_ensemble(year, variable):\n",
    "    context = get_context(year, variable = variable)\n",
    "    datasets = list(map(lambda x: str(get_dataset_url(*x)), context))\n",
    "    return datasets\n",
    "\n",
    "def days_to_ranges(years):\n",
    "    ranges = [year_days[:i +1] for i, n in enumerate(year_days)]\n",
    "    result = [(sum(element[:-1]), sum(element)) for element in ranges]\n",
    "    return result\n",
    "\n",
    "def stack_to_months(stack):\n",
    "    days = days_to_ranges(year_days) if stack.shape[2] == 365 else days_to_ranges(year_leap_days)\n",
    "    for period in days:\n",
    "        yield(stack[:, :, period[0]:period[1], :, :])\n",
    "\n",
    "def stack_to_models(stack):\n",
    "    for model in range(stack.shape[0]):\n",
    "        yield(stack[model, :, :, :])\n",
    "\n",
    "def stack_from_disk(year, chunksize):\n",
    "    year_ensemble_tasmin = list(map(lambda x: x.split('/')[-1], get_year_ensemble(year, variable = 'tasmin')))\n",
    "    year_ensemble_tasmax = list(map(lambda x: x.split('/')[-1], get_year_ensemble(year, variable = 'tasmax')))\n",
    "    datasets_tasmin = [ netCDF4.Dataset('/temp/' + filename) for filename in year_ensemble_tasmin ]\n",
    "    datasets_tasmax = [ netCDF4.Dataset('/temp/' + filename) for filename in year_ensemble_tasmax ]\n",
    "    das_tasmin = list(map(lambda dset: da.from_array(dset['tasmin'], chunks = (365, chunksize, chunksize)), datasets_tasmin))\n",
    "    das_tasmax = list(map(lambda dset: da.from_array(dset['tasmax'], chunks = (365, chunksize, chunksize)), datasets_tasmax))\n",
    "    # da.stack(list(map(lambda dset: da.from_array(dset['tasmax'], chunks = (365, 144, 144)), datasets_tasmax)))\n",
    "    final_stack_tasmin = da.stack(das_tasmin)\n",
    "    final_stack_tasmax = da.stack(das_tasmax)\n",
    "    final_stack = da.stack((final_stack_tasmin, final_stack_tasmax))\n",
    "    return final_stack\n",
    "\n",
    "def download(year):\n",
    "    context_tasmax = get_context(year, variable = 'tasmax')\n",
    "    context_tasmin = get_context(year, variable = 'tasmin')\n",
    "    urls = list(map(lambda ctx: get_dataset_url( *ctx ), [*context_tasmin, *context_tasmax]))\n",
    "    filenames = list(map(lambda url: '/temp/' + url.split('/')[-1],  urls))\n",
    "    with tempfile.NamedTemporaryFile(mode = 'w', delete = False) as download_list:\n",
    "        for url in urls:\n",
    "            download_list.write(url + '\\n')\n",
    "        download_command = f' aria2c -i {download_list.name} --log={download_list.name}.log --log-level=warn --dir /temp --max-tries 5 --retry-wait 5'\n",
    "        print(\"Downloading files\")\n",
    "        download_result = subprocess.Popen(download_command, shell=True, stdout=subprocess.PIPE)\n",
    "    return download_result, filenames\n",
    "\n",
    "def download_and_stack(year):\n",
    "    downloads, filenames = download(year)\n",
    "    result = downloads.wait()\n",
    "    print(result)\n",
    "    if result != 0:\n",
    "        raise Exception('Downloads failed')\n",
    "    print(\"Finished the downloads\")\n",
    "    stack = stack_from_disk(year, 144)\n",
    "    return stack, filenames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Actual processing\n",
    "# We load the temperature baseline from a numpy array in disk\n",
    "baseline = np.load('./baseline_tasmax_99p.npy')\n",
    "client.scatter(baseline)\n",
    "\n",
    "# And define the functions to be applied over each dataset\n",
    "# This if for a single year, either one of the variables or the\n",
    "# tasavg stack, which would be calculated on-the-fly\n",
    "\n",
    "# Heating Degree Days - in C, transformation to F should not be problematic\n",
    "def hdd(a, axis):\n",
    "    a_to_baseline = 291.483 - a\n",
    "    masked = ma.masked_where(a_to_baseline <= 0, a_to_baseline)\n",
    "    intermediate_matrix = ma.filled(masked, fill_value = 0)\n",
    "    result = np.sum(intermediate_matrix, axis = 0)\n",
    "    return result\n",
    "\n",
    "# Cooling degree days\n",
    "def cdd(a, axis):\n",
    "    a_to_baseline = 291.483 - a\n",
    "    a_to_baseline[a_to_baseline < -10000] = 0\n",
    "    masked = ma.masked_where(a_to_baseline >= 0, a_to_baseline)\n",
    "    intermediate_matrix = ma.filled(masked, fill_value = 0)\n",
    "    result = np.sum(np.abs(intermediate_matrix), axis = 0)\n",
    "    return result\n",
    "\n",
    "# Number of days of the year with tasmax > 99 percentile from baseline 1971-2000\n",
    "def extreme_heat(a, axis):\n",
    "    a_to_baseline = a - baseline\n",
    "    masked = ma.masked_where(a_to_baseline <= 0, a_to_baseline)\n",
    "    intermediate_matrix = ma.filled(masked, fill_value = 0)\n",
    "    result = np.count_nonzero(intermediate_matrix, axis = axis)\n",
    "    return result\n",
    "\n",
    "# Helper function, not to be applied directly on the worker\n",
    "def longest_streak(diff):\n",
    "    result = 0\n",
    "    try:\n",
    "        result =  np.amax(\n",
    "            np.array(np.where(diff < 0)) - np.array(np.where(diff > 0))\n",
    "        )\n",
    "    except ValueError:\n",
    "        #raised if empty\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "# Longest streak of days over freezing temperature (tasmin)\n",
    "def frost_free_season(a, axis):\n",
    "    # First, dealing with the first matrix\n",
    "    frost_days_matrix = (a > 273.15) * 1\n",
    "    # We pad it with zeroes at the ends of the designed axis\n",
    "    zeros_shape = list(a.shape)\n",
    "    del zeros_shape[axis]\n",
    "    zeros_matrix = np.expand_dims(np.zeros(zeros_shape), axis = axis)\n",
    "    concat_matrix = np.concatenate((zeros_matrix, frost_days_matrix, zeros_matrix))\n",
    "    # We calculate the deltas along an axis\n",
    "    diff = np.diff(concat_matrix, axis = axis)\n",
    "    # And get the longest streak from there --\n",
    "    # apply along axis is far from ideal, but\n",
    "    # np.where doesn't operate over axes, so we have to iterate\n",
    "    result = np.apply_along_axis(longest_streak, axis, diff)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old files\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Client: scheduler='tcp://172.21.0.2:8786' processes=1 cores=4>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Removing old files\")\n",
    "for netcdf in glob.glob(\"/temp/*.nc*\"):\n",
    "    os.remove(netcdf)\n",
    "print(\"Done.\")\n",
    "\n",
    "client.restart()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing year 1971\n",
      "Downloading files\n"
     ]
    }
   ],
   "source": [
    "for year in [1971, 1973, 1974, 1975, 1977, 1978, 1979, 1981, 1982, 1983, 1985, 1986, 1987, 1999, 2000]:\n",
    "    try:\n",
    "        print(f\"processing year {year}\")\n",
    "        year_stack, _ = download_and_stack(year)\n",
    "        print(year_stack)\n",
    "        \n",
    "        # Yearly indexes - we define the datasets as slices of the in-worker dataset\n",
    "        tasmax_stack = year_stack[0, :, :, :, :]\n",
    "        tasmin_stack = year_stack[1, :, :, :, :]\n",
    "        tasavg_stack = (tasmax_stack + tasmin_stack) / 2\n",
    "        tasmin_per_model = list(stack_to_models(tasmin_stack))\n",
    "        tasmax_per_model = list(stack_to_models(tasmax_stack))\n",
    "        tasavg_per_model = list(stack_to_models(tasavg_stack))\n",
    "\n",
    "        hdds_per_model = list(map(lambda arr: delayed(hdd)(arr, axis=0).compute(), tasavg_per_model))   \n",
    "        hdds_final_stack = np.stack(hdds_per_model)\n",
    "        np.save(f'/results/{year}_hdds_per_model.npy', hdds_final_stack)\n",
    "        \n",
    "        cdds_per_model = list(map(lambda arr: delayed(cdd)(arr, axis=0).compute(), tasavg_per_model))   \n",
    "        cdds_final_stack = np.stack(cdds_per_model)\n",
    "        np.save(f'/results/{year}_cdds_per_model.npy', cdds_final_stack)\n",
    "        \n",
    "        ffs_per_model = list(map(lambda arr: delayed(frost_free_season)(arr, axis=0).compute(), tasmin_per_model))\n",
    "        ffs_final_stack = np.stack(ffs_per_model)\n",
    "        np.save(f'/results/{year}_ffs_per_model.npy', ffs_final_stack)\n",
    "        \n",
    "        xs_per_model = list(map(lambda arr: delayed(extreme_heat)(arr, axis=0).compute(), tasmax_per_model))\n",
    "        xs_final_stack = np.stack(xs_per_model)\n",
    "        np.save(f'/results/{year}_xs_per_model.npy', xs_final_stack)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"There was some error for year {year}\")\n",
    "        print(str(e))\n",
    "        pass\n",
    "    finally:\n",
    "        for netcdf in glob.glob(\"/temp/*.nc*\"):\n",
    "            os.remove(netcdf)\n",
    "    print(f\"Finished with year {year}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "process_temperatures_aria2c.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
