{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.dot import dot_graph\n",
    "import itertools\n",
    "import logging\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "from urllib import request\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "\n",
    "client = Client('scheduler:8786')\n",
    "\n",
    "download_location = '/temp'\n",
    "# data_url = 'http://nasanex.s3.amazonaws.com'\n",
    "data_url = 'http://172.21.0.1:8080'\n",
    "max_download_attempts = 5\n",
    "\n",
    "all_models = ['ACCESS1-0',  'BNU-ESM', 'CCSM4', 'CESM1-BGC', 'CNRM-CM5', 'CSIRO-Mk3-6-0', 'CanESM2', 'GFDL-CM3', 'GFDL-ESM2G', 'GFDL-ESM2M', 'IPSL-CM5A-LR', 'IPSL-CM5A-MR', 'MIROC-ESM-CHEM', 'MIROC-ESM', 'MIROC5', 'MPI-ESM-LR', 'MPI-ESM-MR', 'MRI-CGCM3', 'NorESM1-M', 'bcc-csm1-1', 'inmcm4']\n",
    "# all_models = ['ACCESS1-0', 'BNU-ESM'] \n",
    "all_vars = ['tasmax', 'pr']\n",
    "all_years = {\n",
    "     # 'historical': list(range(1971, 1976))\n",
    "    'historical': list(range(1971, 2001))\n",
    "}\n",
    "\n",
    "year_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "year_leap_days = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\n",
    "def get_dataset_url(variable, scenario, model, year, prefix = data_url):\n",
    "    prefix_filename = '/'.join([prefix, 'NEX-GDDP', 'BCSD', scenario, 'day', 'atmos', variable, 'r1i1p1', 'v1.0'])\n",
    "    filename = '_'.join([variable, 'day', 'BCSD', scenario, 'r1i1p1', model, str(year) + '.nc'])\n",
    "    url = '/'.join([prefix_filename, filename])\n",
    "    # url = '/'.join([prefix, filename])\n",
    "    return url\n",
    "\n",
    "def get_context(year, **kwargs):\n",
    "    variables = [kwargs.get('variable')] if kwargs.get('variable') else all_vars\n",
    "    scenarios = ['historical']\n",
    "    models = [kwargs.get('model')] if kwargs.get('model') else all_models\n",
    "    outlist = []\n",
    "    combinations = list(itertools.product(variables, scenarios, models))\n",
    "    result = list(map(lambda comb: [ *comb, year ], combinations))\n",
    "    return result\n",
    "\n",
    "def get_year_ensemble(year, variable = 'tasmax'):\n",
    "    context = get_context(year, variable = variable)\n",
    "    datasets = list(map(lambda x: str(get_dataset_url(*x)), context))\n",
    "    return datasets\n",
    "\n",
    "def download_file(url):\n",
    "    print(\"url: \" + url)\n",
    "    attempts = 0\n",
    "    success = False\n",
    "    filename = \"\"\n",
    "    while attempts < max_download_attempts and not success:\n",
    "        time.sleep(2 ** attempts)\n",
    "        filename = '/'.join([download_location, str(url.split('/')[-1])])\n",
    "        print(\"Downloading file at \" + filename)\n",
    "        u = request.urlopen(url)\n",
    "        f = open(filename, 'wb')\n",
    "        f.write(u.read())\n",
    "        f.close()\n",
    "        success = True\n",
    "        break\n",
    "    return filename\n",
    "\n",
    "def download_file_list(url_list):\n",
    "    print(\"Starting download pool\")\n",
    "    pool = Pool()\n",
    "    res = pool.map(download_file, url_list)\n",
    "    print(\"Jobs sent\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"Downloads finished\")\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "def download_and_stack(year, variable):\n",
    "    dsets_urls = list(map(lambda x: get_year_ensemble(x, variable = variable), [year]))[0]\n",
    "    filenames = download_file_list(dsets_urls)\n",
    "    datasets = [ netCDF4.Dataset(filename) for filename in filenames ]\n",
    "    dask_arrays = []\n",
    "    for dset in datasets:\n",
    "        dask_arrays.append(da.from_array(dset[str(variable)], chunks= (366, 144, 144)))\n",
    "    try:\n",
    "        final_stack = da.stack(dask_arrays, axis = 0)\n",
    "    except:\n",
    "        return filenames, None, None\n",
    "    return filenames, datasets, final_stack\n",
    "\n",
    "def destack(a, chunksize):\n",
    "    nmodels, tasvars, time, lat, lon = a.shape\n",
    "    nstacks_lat = int(np.ceil(lat / chunksize))\n",
    "    nstacks_lon = int(np.ceil(lon / chunksize))\n",
    "    \n",
    "    stacks = []\n",
    "    \n",
    "    for i in range(nstacks_lat):\n",
    "        for j in range(nstacks_lon):\n",
    "            latmin, latmax = i * chunksize, (i+1) * chunksize\n",
    "            lonmin, lonmax = j * chunksize, (j+1) * chunksize\n",
    "            # print(i, j, '~>', latmin, latmax, lonmin, lonmax)\n",
    "            stacked = a[:, :, :, latmin:latmax, lonmin:lonmax]\n",
    "            # print(stacked)\n",
    "            stacks.append(stacked)\n",
    "    return stacks\n",
    "\n",
    "def copy_dataset(src, output_filename):\n",
    "    dst = netCDF4.Dataset('/home/jovyan/work/' + output_filename, \"w\")\n",
    "    # copy attributes\n",
    "    for name in src.ncattrs():\n",
    "        dst.setncattr(name, src.getncattr(name))\n",
    "    # copy dimensions\n",
    "    for name, dimension in src.dimensions.items():\n",
    "        dst.createDimension(\n",
    "            name, (dimension)\n",
    "        )\n",
    "    dst.close()\n",
    "    return output_filename\n",
    "\n",
    "def restack(chunk_list, chunksize):\n",
    "    shapes = list(map(np.shape, chunk_list))\n",
    "    ndays = shapes[0][0]\n",
    "    nlons = int(1440 / chunksize)\n",
    "    nlats = int(720 / chunksize)\n",
    "    out_array = np.empty((ndays, 720, 1440))\n",
    "    combs = list(itertools.product(\n",
    "        list(range(nlats)),\n",
    "        list(range(nlons))\n",
    "    ))\n",
    "    res_list = zip(combs, chunk_list)\n",
    "    for position, arr in res_list:\n",
    "        minlon, maxlon = position[0] * chunksize, position[0] * chunksize + chunksize\n",
    "        minlat, maxlat = position[1] * chunksize, position[1] * chunksize + chunksize\n",
    "        out_array[:, minlon:maxlon, minlat:maxlat] = arr\n",
    "    return out_array\n",
    "\n",
    "def stack_from_disk(year, chunksize):\n",
    "    year_ensemble_tasmin = list(map(lambda x: x.split('/')[-1], get_year_ensemble(year, variable = 'tasmin')))\n",
    "    year_ensemble_tasmax = list(map(lambda x: x.split('/')[-1], get_year_ensemble(year, variable = 'tasmax')))\n",
    "    datasets_tasmin = [ netCDF4.Dataset('../temp/' + filename) for filename in year_ensemble_tasmin ]\n",
    "    datasets_tasmax = [ netCDF4.Dataset('../temp/' + filename) for filename in year_ensemble_tasmax ]\n",
    "    das_tasmin = list(map(lambda dset: da.from_array(dset['tasmin'], chunks = (365, chunksize, chunksize)), datasets_tasmin))\n",
    "    das_tasmax = list(map(lambda dset: da.from_array(dset['tasmax'], chunks = (365, chunksize, chunksize)), datasets_tasmax))\n",
    "    # da.stack(list(map(lambda dset: da.from_array(dset['tasmax'], chunks = (365, 144, 144)), datasets_tasmax)))\n",
    "    final_stack = da.stack([da.stack(tup) for tup in list(zip(das_tasmin, das_tasmax))])\n",
    "    return final_stack\n",
    "\n",
    "def days_to_ranges(years):\n",
    "    ranges = [year_days[:i +1] for i, n in enumerate(year_days)]\n",
    "    result = [(sum(element[:-1]), sum(element)) for element in ranges]\n",
    "    return result\n",
    "\n",
    "def stack_to_months(stack):\n",
    "    days = days_to_ranges(year_days) if stack.shape[2] == 365 else days_to_ranges(year_leap_days)\n",
    "    for period in days:\n",
    "        yield(stack[:, :, period[0]:period[1], :, :])\n",
    "\n",
    "def stack_to_models(stack):\n",
    "    for model in range(stack.shape[0]):\n",
    "        yield(stack[model, :, :, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Client: scheduler='tcp://172.21.0.2:8786' processes=1 cores=4>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure we have a clean client for testing\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing for year 1971\n",
      "2017-11-27 20:56:29.168995\n",
      "Downloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<stack, shape=(21, 2, 365, 720, 1440), dtype=float32, chunksize=(1, 1, 365, 144, 144)>\n"
     ]
    }
   ],
   "source": [
    "year = 1971\n",
    "\n",
    "print(f\"Starting processing for year {str(year)}\")\n",
    "print(datetime.datetime.now())\n",
    "print(\"Downloading\")\n",
    "\n",
    "# Stacking all data into the same dask dataset\n",
    "# Change for download from amazon\n",
    "stack_year = stack_from_disk(year, 144)\n",
    "# Geographic chunking\n",
    "print(stack_year)\n",
    "# dask.array<stack, shape=(21, 2, 365, 720, 1440), dtype=float32, chunksize=(1, 1, 365, 360, 360)>\n",
    "# tasavg = np.mean(stack_year, axis = 1)\n",
    "# Can't really calc this without flooding memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<getitem, shape=(21, 365, 720, 1440), dtype=float32, chunksize=(1, 365, 144, 144)>\n",
      "dask.array<getitem, shape=(21, 365, 720, 1440), dtype=float32, chunksize=(1, 365, 144, 144)>\n",
      "dask.array<truediv, shape=(21, 365, 720, 1440), dtype=float32, chunksize=(1, 365, 144, 144)>\n",
      "[dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>, dask.array<getitem, shape=(365, 720, 1440), dtype=float32, chunksize=(365, 144, 144)>]\n"
     ]
    }
   ],
   "source": [
    "# Yearly indexes:\n",
    "\n",
    "tasmax_stack = stack_year[:, 0, :, :, :]\n",
    "tasmin_stack = stack_year[:, 1, :, :, :]\n",
    "\n",
    "print(tasmax_stack)\n",
    "print(tasmin_stack)\n",
    "\n",
    "tasavg_stack = (tasmax_stack + tasmin_stack) / 2\n",
    "print(tasavg_stack)\n",
    "\n",
    "tasavg_per_model = list(stack_to_models(tasavg_stack))\n",
    "print(tasavg_per_model)\n",
    "\n",
    "def cdd(a, axis):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "month_stacks = list(stack_to_months(stack_year))\n",
    "print(\"Processing monthly information\")\n",
    "for i, month in enumerate(month_stacks):\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # Processing temperatures\n",
    "\n",
    "    # Tasmin\n",
    "    tasmin_arr = month[:, 0, :, :, :]\n",
    "    print(f\"tasmin_arr.shape: {tasmin_arr.shape}\")\n",
    "    # tasmin_arr.shape: (21, 31, 720, 1440)    \n",
    "\n",
    "    # In this case this axis corresponds to days\n",
    "    base_tasmin_mean = np.mean(tasmin_arr, axis = 1)\n",
    "    # So we first compute an average across days of the month, per model\n",
    "\n",
    "    # And the mean and percentile per model\n",
    "    tasmin_avg = np.mean(base_tasmin_mean, axis = 0).compute()\n",
    "    print(tasmin_avg.shape)\n",
    "    # (720, 1440)\n",
    "\n",
    "    np.save('/temp/tasmin_avg_1971.npy', tasmin_avg)\n",
    "    \n",
    "    tasmin_p75 = np.percentile(base_tasmin_mean, 75, axis = 0)\n",
    "    print(tasmin_p75.shape)\n",
    "    # (720, 1440)\n",
    "\n",
    "    tasmin_p25 = np.percentile(base_tasmin_mean, 25, axis = 0)\n",
    "    print(tasmin_p25.shape)\n",
    "    # (720, 1440)\n",
    "\n",
    "    np.save('/temp/tasmin_p75_1971.npy', tasmin_p75)\n",
    "    np.save('/temp/tasmin_p25_1971.npy', tasmin_p25)\n",
    "\n",
    "    # Tasmax\n",
    "    # tasmax_arr = month[:, 1, :, :, :]\n",
    "    # print(f\"tasmax_arr.shape: {tasmax_arr.shape}\")\n",
    "\n",
    "    # base_tasmax_mean = np.mean(tasmax_arr, axis = 1)\n",
    "    # tasmax_avg = np.mean(base_tasmax_mean, axis = 0).compute()\n",
    "    # print(tasmax_avg.shape)\n",
    "    # tasmax_p75 = np.percentile(base_tasmax_mean, 75, axis = 0)\n",
    "    # print(tasmax_avg.shape)\n",
    "    # tasmax_p25 = np.percentile(base_tasmax_mean, 25, axis = 0)\n",
    "    # print(tasmax_avg.shape)\n",
    "\n",
    "    # tasavg_arr = (tasmin_arr + tasmax_arr) / 2\n",
    "    # print(tasavg_arr)\n",
    "    # base_tasavg_mean = np.mean(tasavg_arr, axis = 0)\n",
    "    # print(base_tasavg_mean)\n",
    "    # tasavg_avg = np.mean(base_tasavg_mean, axis = 0).compute()\n",
    "    # print(tasavg_avg.shape)\n",
    "    # tasavg_p75 = np.percentile(base_tasavg_mean, 75, axis = 0)\n",
    "    # print(tasavg_p75.shape)\n",
    "    # tasavg_p25 = np.percentile(base_tasavg_mean, 25, axis = 0)\n",
    "    # print(tasavg_p25.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "process_temperatures.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
