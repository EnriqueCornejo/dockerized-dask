{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.dot import dot_graph\n",
    "import itertools\n",
    "import logging\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "from urllib import request\n",
    "from multiprocessing import Pool\n",
    "\n",
    "client = Client('scheduler:8786')\n",
    "\n",
    "download_location = '/temp'\n",
    "# data_url = 'http://nasanex.s3.amazonaws.com'\n",
    "data_url = 'http://192.168.1.123:8080'\n",
    "max_download_attempts = 5\n",
    "\n",
    "all_models = ['ACCESS1-0',  'BNU-ESM', 'CCSM4', 'CESM1-BGC', 'CNRM-CM5', 'CSIRO-Mk3-6-0', 'CanESM2', 'GFDL-CM3', 'GFDL-ESM2G', 'GFDL-ESM2M', 'IPSL-CM5A-LR', 'IPSL-CM5A-MR', 'MIROC-ESM-CHEM', 'MIROC-ESM', 'MIROC5', 'MPI-ESM-LR', 'MPI-ESM-MR', 'MRI-CGCM3', 'NorESM1-M', 'bcc-csm1-1', 'inmcm4']\n",
    "# all_models = ['ACCESS1-0', 'BNU-ESM'] \n",
    "all_vars = ['tasmax', 'pr']\n",
    "all_years = {\n",
    "     # 'historical': list(range(1971, 1976))\n",
    "     # 'historical': list(range(1971, 2001))\n",
    "    'historical': [1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000]\n",
    "}\n",
    "\n",
    "def get_dataset_url(variable, scenario, model, year, prefix = data_url):\n",
    "    prefix_filename = '/'.join([prefix, 'NEX-GDDP', 'BCSD', scenario, 'day', 'atmos', variable, 'r1i1p1', 'v1.0'])\n",
    "    filename = '_'.join([variable, 'day', 'BCSD', scenario, 'r1i1p1', model, str(year) + '.nc'])\n",
    "    url = '/'.join([prefix_filename, filename])\n",
    "    return url\n",
    "\n",
    "def get_context(year, **kwargs):\n",
    "    variables = [kwargs.get('variable')] if kwargs.get('variable') else all_vars\n",
    "    scenarios = ['historical']\n",
    "    models = [kwargs.get('model')] if kwargs.get('model') else all_models\n",
    "    outlist = []\n",
    "    combinations = list(itertools.product(variables, scenarios, models))\n",
    "    result = list(map(lambda comb: [ *comb, year ], combinations))\n",
    "    return result\n",
    "\n",
    "def get_year_ensemble(year, variable = 'tasmax'):\n",
    "    context = get_context(year, variable = variable)\n",
    "    datasets = list(map(lambda x: str(get_dataset_url(*x)), context))\n",
    "    return datasets\n",
    "\n",
    "def download_file(url):\n",
    "    print(\"url: \" + url)\n",
    "    attempts = 0\n",
    "    success = False\n",
    "    filename = \"\"\n",
    "    while attempts < max_download_attempts and not success:\n",
    "        time.sleep(2 ** attempts)\n",
    "        filename = '/'.join([download_location, str(url.split('/')[-1])])\n",
    "        print(\"Downloading file at \" + filename)\n",
    "        u = request.urlopen(url)\n",
    "        f = open(filename, 'wb')\n",
    "        f.write(u.read())\n",
    "        f.close()\n",
    "        success = True\n",
    "        break\n",
    "    return filename\n",
    "\n",
    "def download_file_list(url_list):\n",
    "    print(\"Starting download pool\")\n",
    "    pool = Pool()\n",
    "    res = pool.map(download_file, url_list)\n",
    "    print(\"Jobs sent\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"Downloads finished\")\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "def download_and_stack(year, variable):\n",
    "    dsets_urls = list(map(lambda x: get_year_ensemble(x, variable = variable), [year]))[0]\n",
    "    filenames = download_file_list(dsets_urls)\n",
    "    datasets = [ netCDF4.Dataset(filename) for filename in filenames ]\n",
    "    dask_arrays = []\n",
    "    for dset in datasets:\n",
    "        dask_array = da.from_array(dset[variable], chunks = (366, 144, 144))\n",
    "        if dask_array.shape == (366, 720, 1440):\n",
    "            dask_array_noleap = dask_array[[*list(range(0, 60)), *list(range(61, 366))],:,:]\n",
    "            print(dask_array_noleap)\n",
    "            dask_arrays.append(dask_array_noleap)\n",
    "        else:\n",
    "            print(dask_array)\n",
    "            dask_arrays.append(dask_array)\n",
    "    try:\n",
    "        final_stack = da.stack(dask_arrays, axis = 0)\n",
    "    except:\n",
    "        return filenames, None, None\n",
    "    return filenames, datasets, final_stack\n",
    "\n",
    "def get_stacks_mod_avg(a, chunksize):\n",
    "    nmodels, time, lat, lon = a.shape\n",
    "    nstacks_lat = int(np.ceil(lat / chunksize))\n",
    "    nstacks_lon = int(np.ceil(lon / chunksize))\n",
    "    \n",
    "    stacks = []\n",
    "    \n",
    "    for i in range(nstacks_lat):\n",
    "        for j in range(nstacks_lon):\n",
    "            latmin, latmax = i * chunksize, (i+1) * chunksize\n",
    "            lonmin, lonmax = j * chunksize, (j+1) * chunksize\n",
    "            print(i, j, '~>', latmin, latmax, lonmin, lonmax)\n",
    "            stacked = a[:, :, latmin:latmax, lonmin:lonmax]\n",
    "            print(stacked)\n",
    "            stacks.append(stacked)\n",
    "    return stacks\n",
    "\n",
    "def copy_dataset(src, output_filename):\n",
    "    dst = netCDF4.Dataset('/home/jovyan/work/' + output_filename, \"w\")\n",
    "    # copy attributes\n",
    "    for name in src.ncattrs():\n",
    "        dst.setncattr(name, src.getncattr(name))\n",
    "    # copy dimensions\n",
    "    for name, dimension in src.dimensions.items():\n",
    "        dst.createDimension(\n",
    "            name, (len(dimension) if not dimension.isunlimited else None))\n",
    "    dst.close()\n",
    "    return output_filename\n",
    "\n",
    "def restack(chunk_list, chunksize):\n",
    "    shapes = list(map(np.shape, chunk_list))\n",
    "    ndays = shapes[0][0]\n",
    "    nlons = int(1440 / chunksize)\n",
    "    nlats = int(720 / chunksize)\n",
    "\n",
    "    out_array = np.empty((ndays, 720, 1440))\n",
    " \n",
    "    combs = list(itertools.product(\n",
    "        list(range(nlats)),\n",
    "        list(range(nlons))\n",
    "    ))\n",
    "    \n",
    "    res_list = zip(combs, chunk_list)\n",
    "    \n",
    "    for position, arr in res_list:\n",
    "        minlon, maxlon = position[0] * chunksize, position[0] * chunksize + chunksize\n",
    "        minlat, maxlat = position[1] * chunksize, position[1] * chunksize + chunksize\n",
    "        out_array[:, minlon:maxlon, minlat:maxlat] = arr\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Processing pr years\n",
    "years = list(range(1971, 2001))\n",
    "years = [1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000]\n",
    "var = 'pr'\n",
    "for i, year in enumerate(years):\n",
    "    filenames, datasets, current_year_stack = download_and_stack(year, variable=var)\n",
    "    if current_year_stack is not None:\n",
    "        chunked_stack = get_stacks_mod_avg(current_year_stack, 360)\n",
    "        mean = list(map(lambda x: delayed(np.mean)(x, axis=0).compute(), chunked_stack))\n",
    "        out_array = restack(mean, 360)\n",
    "        np.save('/home/jovyan/work/' + str(year) + '_pr_avg.npy', out_array)\n",
    "        del chunked_stack\n",
    "        del mean\n",
    "        del out_array\n",
    "        del current_year_stack\n",
    "        #output_dataset_filename = copy_dataset(datasets[0], str(year) + '_ensemble_' + var + '.nc')\n",
    "        #output_dataset = netCDF4.Dataset(output_dataset_filename, 'w')\n",
    "        #print(output_dataset)\n",
    "        \n",
    "    # Finally\n",
    "    for filename in filenames:\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -l /temp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def download_and_stack(year, variable):\n",
    "    dsets_urls = list(map(lambda x: get_year_ensemble(x, variable = variable), [year]))[0]\n",
    "    filenames = download_file_list(dsets_urls)\n",
    "    datasets = [ netCDF4.Dataset(filename) for filename in filenames ]\n",
    "    dask_arrays = []\n",
    "    for dset in datasets:\n",
    "        dask_array = da.from_array(dset[variable], chunks = (366, 144, 144))\n",
    "        print(dask_array)\n",
    "        if dask_array.shape == (366, 720, 1440):\n",
    "            dask_array_noleap = dask_array[[*list(range(0, 60)), *list(range(61, 366))],:,:]\n",
    "            print(dask_array_noleap)\n",
    "            dask_arrays.append(dask_array_noleap)\n",
    "        else:\n",
    "            dask_arrays.append(dask_array)\n",
    "    try:\n",
    "        final_stack = da.stack(dask_arrays, axis = 0)\n",
    "    except:\n",
    "        return filenames, None, None\n",
    "    return filenames, datasets, final_stack\n",
    "\n",
    "download_and_stack(1972, 'pr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm /temp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls /temp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "year_matrixes = sorted([fname for fname in glob.glob('/temp/*.npy')])\n",
    "year_matrixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = np.load('/temp/1997_pr_avg.npy')\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def slice_constructor(shape, axis, chunksize):\n",
    "    out = []\n",
    "    shape_list = list(shape)\n",
    "    del shape_list[axis]\n",
    "    \n",
    "    def slicer(*args):\n",
    "        slices = []\n",
    "        for arg, dim in zip(args, shape_list):\n",
    "            slices.append(slice(arg * chunksize, (arg + 1) * chunksize))\n",
    "        slices.insert(axis, slice(0, shape[axis] ))\n",
    "        return slices\n",
    "    return slicer\n",
    "\n",
    "def apply_by_chunks(f, axis, arrs, arrsize, chunksize, **kwargs):\n",
    "    slicer = slice_constructor(arrsize, axis, chunksize)\n",
    "    total_results = []\n",
    "    for lat in range(0, 10):\n",
    "        for lon in range(0, 20):\n",
    "            print(\"processing \" + str(lat) + \" \" + str(lon))\n",
    "            slice = slicer(lat, lon)\n",
    "            print(slice)\n",
    "            \n",
    "            current_processing_stack = []\n",
    "            for arr_filename in arrs:\n",
    "                print(\"Processing \" + arr_filename)\n",
    "                original_matrix = np.load(arr_filename)\n",
    "                copied_slice = np.array(original_matrix[slice], copy=True)\n",
    "                del original_matrix\n",
    "                current_processing_stack.append(copied_slice)\n",
    "                # print(current_processing_stack)\n",
    "                # print(current_processing_stack.shape)\n",
    "            print(\"Stacking\")\n",
    "            final_stack = np.concatenate(current_processing_stack, axis = axis)\n",
    "            np.save('/temp/' + str(lat) + '_' + str(lon), final_stack)\n",
    "            print(\"Calculating\")\n",
    "            result = f(final_stack, axis = axis, **kwargs)\n",
    "            np.save('/temp/result_' + str(lat) + '_' + str(lon), result)\n",
    "            print(result)\n",
    "            print(result.shape)\n",
    "            total_results.append(result)\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "apply_by_chunks(np.percentile, 0, year_matrixes, (365, 720, 1440), 72, q=99)\n",
    "# f = slice_constructor((365, 720, 1440), 0, 72)\n",
    "# [slice(0, 365, None), slice(360, 432, None), slice(504, 576, None)]\n",
    "# f(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
